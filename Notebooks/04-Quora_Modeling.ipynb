{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGihjEIsXZvk",
        "outputId": "1c2ef468-4ba8-4117-b317-f384cf2d65fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, log_loss,f1_score,accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import mlflow\n",
        "#import mlflow.sklearn\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCuvoRpGNGz_"
      },
      "outputs": [],
      "source": [
        "#! pip install mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMr7b89PpsVL",
        "outputId": "63a5252f-d0b7-4a9e-99ae-a56fd12dd109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM1-lSoLoXYr"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/quora/maintraindf.csv'\n",
        "test_path = '/content/drive/MyDrive/quora/maintestdf.csv'\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GRPAeZ04rC3"
      },
      "outputs": [],
      "source": [
        "train_df['clean_text1'] = train_df['clean_text1'].astype(str)\n",
        "train_df['clean_text2'] = train_df['clean_text2'].astype(str)\n",
        "\n",
        "test_df['clean_text1'] = test_df['clean_text1'].astype(str)\n",
        "test_df['clean_text2'] = test_df['clean_text2'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUTmFvNj4Tnl"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_df['question1_token'] = train_df.clean_text1.apply(word_tokenize)\n",
        "train_df['question2_token'] = train_df.clean_text2.apply(word_tokenize)\n",
        "\n",
        "\n",
        "test_df['question1_token'] = test_df.clean_text1.apply(word_tokenize)\n",
        "test_df['question2_token'] = test_df.clean_text2.apply(word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCjqJzGwjN1u",
        "outputId": "05cf8e4d-b605-4f44-8291-5a71e78c1f36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(323432, 36)\n",
            "(80858, 36)\n",
            "Index(['percentage_common_tokens', 'question1_length', 'question2_length',\n",
            "       'length_difference', 'num_capital_letters1', 'num_capital_letters2',\n",
            "       'num_question_marks1', 'num_question_marks2', 'starts_with_are',\n",
            "       'starts_with_can', 'starts_with_how', 'clean_text1', 'clean_text2',\n",
            "       'word_count1', 'word_count2', 'sentence_count1', 'sentence_count2',\n",
            "       'avg_word_length1', 'avg_word_length2', 'unique_word_count',\n",
            "       'similar_word_count', 'fuzzy_word_partial_ratio', 'token_set_ratio',\n",
            "       'token_sort_ratio', 'word_overlap', 'jaccard_similarity',\n",
            "       'levenshtein_distance', 'length_ratio', 'common_2grams',\n",
            "       'common_3grams', 'average_word_frequency1', 'average_word_frequency2',\n",
            "       'average_word_frequency_diff', 'is_duplicate', 'question1_token',\n",
            "       'question2_token'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(train_df.shape)\n",
        "print(test_df.shape)\n",
        "print(train_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHAm8xMAbL11"
      },
      "outputs": [],
      "source": [
        "# Dropping Unnecessary Columns\n",
        "drop_columns =['clean_text1','clean_text2']\n",
        "train_df.drop(drop_columns,axis=1,inplace=True)\n",
        "test_df.drop(drop_columns,axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZTb3ScRMsyC",
        "outputId": "097681f3-e631-4ba6-c064-6bd10838523e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['percentage_common_tokens', 'question1_length', 'question2_length',\n",
              "       'length_difference', 'num_capital_letters1', 'num_capital_letters2',\n",
              "       'num_question_marks1', 'num_question_marks2', 'starts_with_are',\n",
              "       'starts_with_can', 'starts_with_how', 'word_count1', 'word_count2',\n",
              "       'sentence_count1', 'sentence_count2', 'avg_word_length1',\n",
              "       'avg_word_length2', 'unique_word_count', 'similar_word_count',\n",
              "       'fuzzy_word_partial_ratio', 'token_set_ratio', 'token_sort_ratio',\n",
              "       'word_overlap', 'jaccard_similarity', 'levenshtein_distance',\n",
              "       'length_ratio', 'common_2grams', 'common_3grams',\n",
              "       'average_word_frequency1', 'average_word_frequency2',\n",
              "       'average_word_frequency_diff', 'is_duplicate', 'question1_token',\n",
              "       'question2_token'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIC8ndniMpz3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2kzf7WebNze"
      },
      "outputs": [],
      "source": [
        "tokens = train_df['question1_token'].tolist() + train_df['question2_token'].tolist() + test_df['question1_token'].tolist() + test_df['question2_token'].tolist()\n",
        "word2vec_model = Word2Vec(tokens, window=5, vector_size=200,min_count=1, workers=4)\n",
        "\n",
        "# Function to calculate word embeddings for a sentence\n",
        "def embedding(tokens):\n",
        "    embeddings = [word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv]\n",
        "    if len(embeddings) > 0:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(200)\n",
        "\n",
        "train_df['embedding_question1'] = train_df['question1_token'].apply(embedding)\n",
        "train_df['embedding_question2'] = train_df['question2_token'].apply(embedding)\n",
        "\n",
        "\n",
        "test_df['embedding_question1'] = test_df['question1_token'].apply(embedding)\n",
        "test_df['embedding_question2'] = test_df['question2_token'].apply(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxwVygw7kC2g"
      },
      "outputs": [],
      "source": [
        "def cos_similarity(row):\n",
        "    embedding1 = row['embedding_question1']\n",
        "    embedding2 = row['embedding_question2']\n",
        "    similarity_score = cosine_similarity([embedding1], [embedding2])[0][0]\n",
        "    return similarity_score\n",
        "\n",
        "train_df['cos_similarity'] = train_df.apply(cos_similarity, axis=1)\n",
        "test_df['cos_similarity'] = test_df.apply(cos_similarity, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAo2NQGwdgar"
      },
      "outputs": [],
      "source": [
        "y_train = train_df['is_duplicate']\n",
        "y_test = test_df['is_duplicate']\n",
        "\n",
        "columns_to_exclude = ['embedding_question1', 'embedding_question2']  # Replace with the actual column names\n",
        "X_train = train_df.drop(['is_duplicate','question1_token','question2_token'], axis=1)\n",
        "X_test = test_df.drop(['is_duplicate','question1_token','question2_token'],axis=1)\n",
        "# Separate columns for feature scaling\n",
        "columns_to_scale = [col for col in X_train.columns if col not in columns_to_exclude]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "\n",
        "train_quest1vec = pd.DataFrame(X_train['embedding_question1'].tolist(), columns=[f'q1_{i}' for i in range(200)])\n",
        "train_quest2vec = pd.DataFrame(X_train['embedding_question2'].tolist(), columns=[f'q2_{i}' for i in range(200)])\n",
        "\n",
        "test_quest1vec = pd.DataFrame(X_test['embedding_question1'].tolist(), columns=[f'q1_{i}' for i in range(200)])\n",
        "test_quest2vec = pd.DataFrame(X_test['embedding_question2'].tolist(), columns=[f'q2_{i}' for i in range(200)])\n",
        "\n",
        "train_scaled_columns = scaler.fit_transform(X_train[columns_to_scale])\n",
        "test_scaled_columns = scaler.transform(X_test[columns_to_scale])\n",
        "train_scaled_data = pd.DataFrame(train_scaled_columns, columns=[f'scaled_{i}' for i in range(len(columns_to_scale))])\n",
        "test_scaled_data = pd.DataFrame(test_scaled_columns, columns=[f'scaled_{i}' for i in range(len(columns_to_scale))])\n",
        "\n",
        "X_train = pd.concat([train_quest1vec, train_quest2vec, train_scaled_data], axis=1)\n",
        "X_test = pd.concat([test_quest1vec, test_quest2vec, test_scaled_data], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WukEdkNn8_Zi"
      },
      "outputs": [],
      "source": [
        "# Reset the indices of X_train and y_train\n",
        "trainx = X_train.reset_index(drop=True)\n",
        "trainy = y_train.reset_index(drop=True)\n",
        "\n",
        "# Concatenate X_train with y_train\n",
        "train_df_2 = pd.concat([trainx, trainy], axis=1)\n",
        "\n",
        "# Reset the indices of X_test and y_test\n",
        "testx = X_test.reset_index(drop=True)\n",
        "testy = y_test.reset_index(drop=True)\n",
        "\n",
        "# Concatenate X_test with y_test\n",
        "test_df_2 = pd.concat([testx, testy], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S0r7mTr9u4t",
        "outputId": "8c514fb0-d203-432e-d509-0207965a6518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(323432, 433)\n",
            "(80858, 433)\n"
          ]
        }
      ],
      "source": [
        "print(train_df_2.shape)\n",
        "print(test_df_2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owq5wS2m9WbT"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/quora') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eH0XOTP9hFa"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/quora/trainembeddings.csv'\n",
        "test_path = '/content/drive/MyDrive/quora/testembeddings.csv'\n",
        "train_df_2.to_csv(train_path, index=False)\n",
        "test_df_2.to_csv(test_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYCSIyDd6-zE",
        "outputId": "5cb3de36-738c-4b16-a69a-7ce4ec0c362b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(323432, 432)\n",
            "(80858, 432)\n",
            "Index(['q1_0', 'q1_1', 'q1_2', 'q1_3', 'q1_4', 'q1_5', 'q1_6', 'q1_7', 'q1_8',\n",
            "       'q1_9',\n",
            "       ...\n",
            "       'scaled_22', 'scaled_23', 'scaled_24', 'scaled_25', 'scaled_26',\n",
            "       'scaled_27', 'scaled_28', 'scaled_29', 'scaled_30', 'scaled_31'],\n",
            "      dtype='object', length=432)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_train.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdIm24h8P-dt",
        "outputId": "e97cbde1-d3c2-48f5-811f-b223b89fb9d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['percentage_common_tokens',\n",
              " 'question1_length',\n",
              " 'question2_length',\n",
              " 'length_difference',\n",
              " 'num_capital_letters1',\n",
              " 'num_capital_letters2',\n",
              " 'num_question_marks1',\n",
              " 'num_question_marks2',\n",
              " 'starts_with_are',\n",
              " 'starts_with_can',\n",
              " 'starts_with_how',\n",
              " 'word_count1',\n",
              " 'word_count2',\n",
              " 'sentence_count1',\n",
              " 'sentence_count2',\n",
              " 'avg_word_length1',\n",
              " 'avg_word_length2',\n",
              " 'unique_word_count',\n",
              " 'similar_word_count',\n",
              " 'fuzzy_word_partial_ratio',\n",
              " 'token_set_ratio',\n",
              " 'token_sort_ratio',\n",
              " 'word_overlap',\n",
              " 'jaccard_similarity',\n",
              " 'levenshtein_distance',\n",
              " 'length_ratio',\n",
              " 'common_2grams',\n",
              " 'common_3grams',\n",
              " 'average_word_frequency1',\n",
              " 'average_word_frequency2',\n",
              " 'average_word_frequency_diff',\n",
              " 'cos_similarity']"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "columns_to_scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sozG8nr8C-AW"
      },
      "outputs": [],
      "source": [
        "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
        "\n",
        "mlflow.set_experiment(\"Quora Question Pair Simmilarity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZHP2KK_DhND"
      },
      "outputs": [],
      "source": [
        "from pickle import dump\n",
        "\n",
        "dump(scaler, open('pickle_files/scaler.pkl', 'wb'))\n",
        "dump(word2vec_model, open('pickle_files/word2vec_model.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3lZMmlTfGge",
        "outputId": "0f91fdee-0e38-460a-e96d-5af144c54137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log Loss Test: 0.40124464414832645\n",
            "Log Loss Train: 0.3886427653618916\n",
            "Classification Report Test\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84     50803\n",
            "           1       0.73      0.73      0.73     30055\n",
            "\n",
            "    accuracy                           0.80     80858\n",
            "   macro avg       0.79      0.79      0.79     80858\n",
            "weighted avg       0.80      0.80      0.80     80858\n",
            "\n",
            "******************************************************\n",
            "Classification Report Train\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85    204224\n",
            "           1       0.75      0.74      0.74    119208\n",
            "\n",
            "    accuracy                           0.81    323432\n",
            "   macro avg       0.80      0.80      0.80    323432\n",
            "weighted avg       0.81      0.81      0.81    323432\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "\n",
        "# Initialize the LightGBM model\n",
        "lgm = LGBMClassifier()\n",
        "\n",
        "# Fit the model on the training data\n",
        "lgm.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_test_proba1 = lgm.predict_proba(X_test)\n",
        "y_pred_train_proba1 = lgm.predict_proba(X_train)\n",
        "\n",
        "# # Predict classes on the test set\n",
        "y_test_pred1 = lgm.predict(X_test)\n",
        "y_train_pred1 = lgm.predict(X_train)\n",
        "\n",
        "# Calculate log loss\n",
        "log_loss_test_score1 = log_loss(y_test, y_pred_test_proba1)\n",
        "log_loss_train_score1 = log_loss(y_train, y_pred_train_proba1)\n",
        "\n",
        "# Generate the classification report\n",
        "f1score_test_1 = f1_score(y_test, y_test_pred1)\n",
        "f1score_train_1 = f1_score(y_train, y_train_pred1)\n",
        "\n",
        "accuracy_test_1 = accuracy_score(y_test, y_test_pred1)\n",
        "accuracy_train_1 = accuracy_score(y_train, y_train_pred1)\n",
        "\n",
        "classificationtest = classification_report(y_test, y_test_pred1)\n",
        "classificationtrain = classification_report(y_train, y_train_pred1)\n",
        "\n",
        "print('Log Loss Test:',log_loss_test_score1)\n",
        "print('Log Loss Train:',log_loss_train_score1)\n",
        "\n",
        "print('Classification Report Test\\n', classificationtest)\n",
        "print('******************************************************')\n",
        "print('Classification Report Train\\n', classificationtrain)\n",
        "  # mlflow.log_metric(\"Log Loss\", log_loss_test_score1)\n",
        "  # mlflow.log_metric(\"F1 Score\", f1score_test_1)\n",
        "  # mlflow.log_metric(\"Accuracy Score\", accuracy_test_1)\n",
        "  # mlflow.sklearn.log_model(lgm, artifact_path=\"models\")\n",
        "  # mlflow.log_artifact(\"pickle_files/scaler.pkl\")\n",
        "  # mlflow.log_artifact(\"pickle_files/word2vec_model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7GTLyroePbB",
        "outputId": "aec39eb5-f748-4fe8-d6fc-ee8f2bfeaf26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log Loss Test: 6.924440209193455\n",
            "Log Loss Train: 6.973512179798788\n",
            "Classification Report Test\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.60      0.70     50803\n",
            "           1       0.54      0.81      0.65     30055\n",
            "\n",
            "    accuracy                           0.67     80858\n",
            "   macro avg       0.69      0.70      0.67     80858\n",
            "weighted avg       0.73      0.67      0.68     80858\n",
            "\n",
            "******************************************************\n",
            "Classification Report Train\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.60      0.70    204224\n",
            "           1       0.54      0.81      0.64    119208\n",
            "\n",
            "    accuracy                           0.67    323432\n",
            "   macro avg       0.69      0.70      0.67    323432\n",
            "weighted avg       0.73      0.67      0.68    323432\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# with mlflow.start_run():\n",
        "#   mlflow.set_tag(\"dev\", \"NIKAvengers\")\n",
        "#   mlflow.set_tag(\"algo\", \"Naive Bayes\")\n",
        "\n",
        "# Initialize the Naive Bayes model\n",
        "naivebayes = GaussianNB()\n",
        "\n",
        "# Fit the model on the training data\n",
        "naivebayes.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_test_proba2 = naivebayes.predict_proba(X_test)\n",
        "y_pred_train_proba2 = naivebayes.predict_proba(X_train)\n",
        "\n",
        "# Predict classes on the test set\n",
        "y_test_pred2 = naivebayes.predict(X_test)\n",
        "y_train_pred2 = naivebayes.predict(X_train)\n",
        "\n",
        "# Calculate log loss\n",
        "log_loss_test_score2 = log_loss(y_test, y_pred_test_proba2)\n",
        "log_loss_train_score2 = log_loss(y_train, y_pred_train_proba2)\n",
        "\n",
        "# Generate the classification report\n",
        "f1score_test_2 = f1_score(y_test, y_test_pred2)\n",
        "f1score_train_2 = f1_score(y_train, y_train_pred2)\n",
        "\n",
        "accuracy_test_2 = accuracy_score(y_test, y_test_pred2)\n",
        "accuracy_train_2 = accuracy_score(y_train, y_train_pred2)\n",
        "\n",
        "classificationtest2 = classification_report(y_test, y_test_pred2)\n",
        "classificationtrain2 = classification_report(y_train, y_train_pred2)\n",
        "\n",
        "print('Log Loss Test:',log_loss_test_score2)\n",
        "print('Log Loss Train:',log_loss_train_score2)\n",
        "\n",
        "print('Classification Report Test\\n', classificationtest2)\n",
        "print('******************************************************')\n",
        "print('Classification Report Train\\n', classificationtrain2)\n",
        "  \n",
        "  # mlflow.log_metric(\"Log Loss\", log_loss_test_score2)\n",
        "  # mlflow.log_metric(\"F1 Score\", f1score_test_2)\n",
        "  # mlflow.log_metric(\"Accuracy Score\", accuracy_test_2)\n",
        "  # mlflow.sklearn.log_model(naivebayes, artifact_path=\"models\")\n",
        "  # mlflow.log_artifact(\"pickle_files/scaler.pkl\")\n",
        "  # mlflow.log_artifact(\"pickle_files/word2vec_model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPopeTEsp7yW",
        "outputId": "85330fb0-ecec-49e6-f5d7-f741e53dff40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log Loss Test: 0.3787669353437042\n",
            "Log Loss Train: 0.3301877660356633\n",
            "Classification Report Test\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85     50803\n",
            "           1       0.75      0.74      0.75     30055\n",
            "\n",
            "    accuracy                           0.81     80858\n",
            "   macro avg       0.80      0.80      0.80     80858\n",
            "weighted avg       0.81      0.81      0.81     80858\n",
            "\n",
            "******************************************************\n",
            "Classification Report Train\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.89      0.88    204224\n",
            "           1       0.80      0.78      0.79    119208\n",
            "\n",
            "    accuracy                           0.85    323432\n",
            "   macro avg       0.84      0.84      0.84    323432\n",
            "weighted avg       0.85      0.85      0.85    323432\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# with mlflow.start_run():\n",
        "#   mlflow.set_tag(\"dev\", \"NIKAvengers\")\n",
        "#   mlflow.set_tag(\"algo\", \"XGBoost\")\n",
        "#   # Initialize the XGBoost model\n",
        "xg_boost = xgb.XGBClassifier()\n",
        "\n",
        "# Fit the model on the training data\n",
        "xg_boost.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_test_proba3 = xg_boost.predict_proba(X_test)\n",
        "y_pred_train_proba3 = xg_boost.predict_proba(X_train)\n",
        "\n",
        "# Predict classes on the test set\n",
        "y_test_pred3 = xg_boost.predict(X_test)\n",
        "y_train_pred3 = xg_boost.predict(X_train)\n",
        "\n",
        "# Calculate log loss\n",
        "log_loss_test_score3 = log_loss(y_test, y_pred_test_proba3)\n",
        "log_loss_train_score3 = log_loss(y_train, y_pred_train_proba3)\n",
        "\n",
        "# Generate the classification report\n",
        "f1score_test_3 = f1_score(y_test, y_test_pred3)\n",
        "f1score_train_3 = f1_score(y_train, y_train_pred3)\n",
        "\n",
        "accuracy_test_3 = accuracy_score(y_test, y_test_pred3)\n",
        "accuracy_train_3 = accuracy_score(y_train, y_train_pred3)\n",
        "\n",
        "classificationtest3 = classification_report(y_test, y_test_pred3)\n",
        "classificationtrain3 = classification_report(y_train, y_train_pred3)\n",
        "\n",
        "print('Log Loss Test:',log_loss_test_score3)\n",
        "print('Log Loss Train:',log_loss_train_score3)\n",
        "\n",
        "print('Classification Report Test\\n', classificationtest3)\n",
        "print('******************************************************')\n",
        "print('Classification Report Train\\n', classificationtrain3)\n",
        "\n",
        "  \n",
        "  \n",
        "  # mlflow.log_metric(\"Log Loss\", log_loss_test_score3)\n",
        "  # mlflow.log_metric(\"F1 Score\", f1score_test_3)\n",
        "  # mlflow.log_metric(\"Accuracy Score\", accuracy_test_3)\n",
        "  # mlflow.sklearn.log_model(xg_boost, artifact_path=\"models\")\n",
        "  # mlflow.log_artifact(\"pickle_files/scaler.pkl\")\n",
        "  # mlflow.log_artifact(\"pickle_files/word2vec_model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOX8SDvyJOj-",
        "outputId": "e369b92a-c90a-4cfc-d676-977416de7a18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'boosting_type': 'gbdt',\n",
              " 'class_weight': None,\n",
              " 'colsample_bytree': 1.0,\n",
              " 'importance_type': 'split',\n",
              " 'learning_rate': 0.1,\n",
              " 'max_depth': -1,\n",
              " 'min_child_samples': 20,\n",
              " 'min_child_weight': 0.001,\n",
              " 'min_split_gain': 0.0,\n",
              " 'n_estimators': 100,\n",
              " 'n_jobs': -1,\n",
              " 'num_leaves': 31,\n",
              " 'objective': None,\n",
              " 'random_state': None,\n",
              " 'reg_alpha': 0.0,\n",
              " 'reg_lambda': 0.0,\n",
              " 'silent': 'warn',\n",
              " 'subsample': 1.0,\n",
              " 'subsample_for_bin': 200000,\n",
              " 'subsample_freq': 0}"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lgm.get_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3Urm2TYOqd-",
        "outputId": "6e63fdbe-03a6-4ed2-f262-2ddee1ffa5cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'objective': 'binary:logistic',\n",
              " 'use_label_encoder': None,\n",
              " 'base_score': None,\n",
              " 'booster': None,\n",
              " 'callbacks': None,\n",
              " 'colsample_bylevel': None,\n",
              " 'colsample_bynode': None,\n",
              " 'colsample_bytree': None,\n",
              " 'early_stopping_rounds': None,\n",
              " 'enable_categorical': False,\n",
              " 'eval_metric': None,\n",
              " 'feature_types': None,\n",
              " 'gamma': None,\n",
              " 'gpu_id': None,\n",
              " 'grow_policy': None,\n",
              " 'importance_type': None,\n",
              " 'interaction_constraints': None,\n",
              " 'learning_rate': None,\n",
              " 'max_bin': None,\n",
              " 'max_cat_threshold': None,\n",
              " 'max_cat_to_onehot': None,\n",
              " 'max_delta_step': None,\n",
              " 'max_depth': None,\n",
              " 'max_leaves': None,\n",
              " 'min_child_weight': None,\n",
              " 'missing': nan,\n",
              " 'monotone_constraints': None,\n",
              " 'n_estimators': 100,\n",
              " 'n_jobs': None,\n",
              " 'num_parallel_tree': None,\n",
              " 'predictor': None,\n",
              " 'random_state': None,\n",
              " 'reg_alpha': None,\n",
              " 'reg_lambda': None,\n",
              " 'sampling_method': None,\n",
              " 'scale_pos_weight': None,\n",
              " 'subsample': None,\n",
              " 'tree_method': None,\n",
              " 'validate_parameters': None,\n",
              " 'verbosity': None}"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xg_boost.get_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T6PTImrNqgY9",
        "outputId": "3ec26112-985a-4e66-f0fa-74b1187c28d9"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ce651c7392e9>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# Fit the model on the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrandomforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# Predict probabilities on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "  # Initialize the Random Forest model\n",
        "randomforest = RandomForestClassifier()\n",
        "\n",
        "  # Fit the model on the training data\n",
        "randomforest.fit(X_train, y_train)\n",
        "\n",
        "  # Predict probabilities on the test set\n",
        "y_pred_test_proba4 = randomforest.predict_proba(X_test)\n",
        "y_pred_train_proba4 = randomforest.predict_proba(X_train)\n",
        "\n",
        "  # Predict classes on the test set\n",
        "y_test_pred4 = randomforest.predict(X_test)\n",
        "y_train_pred4 = randomforest.predict(X_train)\n",
        "\n",
        "  # Calculate log loss\n",
        "log_loss_test_score4 = log_loss(y_test, y_pred_test_proba4)\n",
        "log_loss_train_score4 = log_loss(y_train, y_pred_train_proba4)\n",
        "\n",
        "  # Generate the classification report\n",
        "f1score_test_4 = f1_score(y_test, y_test_pred4)\n",
        "f1score_train_4 = f1_score(y_train, y_train_pred4)\n",
        "\n",
        "accuracy_test_4 = accuracy_score(y_test, y_test_pred4)\n",
        "accuracy_train_4 = accuracy_score(y_train, y_train_pred4)\n",
        "\n",
        "\n",
        "classificationtest4= classification_report(y_test, y_test_pred4)\n",
        "classificationtrain4= classification_report(y_train, y_train_pred4)\n",
        "\n",
        "print('Log Loss Test:',log_loss_test_score4)\n",
        "print('Log Loss Train:',log_loss_train_score4)\n",
        "\n",
        "print('Classification Report Test\\n', classificationtest4)\n",
        "print('******************************************************')\n",
        "print('Classification Report Train\\n', classificationtrain4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSHYqz0fxlLJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "with mlflow.start_run():\n",
        "    mlflow.set_tag(\"dev\", \"NIKAvengers\")\n",
        "    mlflow.set_tag(\"algo\", \"GradientBoostingClassifier\")\n",
        "\n",
        "    # Initialize the Gradient Boosting model\n",
        "    gradientboosting = GradientBoostingClassifier()\n",
        "\n",
        "    # Fit the model on the training data\n",
        "    gradientboosting.fit(X_train, y_train)\n",
        "\n",
        "    # Predict probabilities on the test set\n",
        "    y_pred_test_proba5 = gradientboosting.predict_proba(X_test)\n",
        "    y_pred_train_proba5 = gradientboosting.predict_proba(X_train)\n",
        "\n",
        "    # Predict classes on the test set\n",
        "    y_test_pred5 = gradientboosting.predict(X_test)\n",
        "    y_train_pred5 = gradientboosting.predict(X_train)\n",
        "\n",
        "    # Calculate log loss\n",
        "    log_loss_test_score5 = log_loss(y_test, y_pred_test_proba5)\n",
        "    log_loss_train_score5 = log_loss(y_train, y_pred_train_proba5)\n",
        "\n",
        "    # Generate the classification report\n",
        "    f1score_test_5 = f1_score(y_test, y_test_pred5)\n",
        "    f1score_train_5 = f1_score(y_train, y_train_pred5)\n",
        "\n",
        "    accuracy_test_5 = accuracy_score(y_test, y_test_pred5)\n",
        "    accuracy_train_5 = accuracy_score(y_train, y_train_pred5)\n",
        "\n",
        "    mlflow.log_metric(\"Log Loss\", log_loss_test_score5)\n",
        "    mlflow.log_metric(\"F1 Score\", f1score_test_5)\n",
        "    mlflow.log_metric(\"Accuracy Score\", accuracy_test_5)\n",
        "    mlflow.sklearn.log_model(gradientboosting, artifact_path=\"models\")\n",
        "    mlflow.log_artifact(\"pickle_files/scaler.pkl\")\n",
        "    mlflow.log_artifact(\"pickle_files/word2vec_model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Use GPU-enabled version of XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Set the GPU-related parameters for LightGBM\n",
        "lgb_params = {\n",
        "    'device': 'gpu'\n",
        "}\n",
        "\n",
        "# Define the base models\n",
        "base_models = [\n",
        "    ('lgm', lgb.LGBMClassifier()),\n",
        "    ('xgb', XGBClassifier(tree_method='gpu_hist')),\n",
        "    ('randomforest', RandomForestClassifier())\n",
        "]"
      ],
      "metadata": {
        "id": "2dEqtF5wsFyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Train the stacking classifier\n",
        "stacking_classifier = StackingClassifier(estimators=base_models, final_estimator=XGBClassifier(tree_method='gpu_hist'))\n",
        "stacking_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = stacking_classifier.predict(X_test)\n",
        "y_pred_train = stacking_classifier.predict(X_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_proba = stacking_classifier.predict_proba(X_test)\n",
        "y_pred_proba_train = stacking_classifier.predict_proba(X_train)\n",
        "\n",
        "# Calculate log loss\n",
        "log_loss_score = log_loss(y_test, y_pred_proba)\n",
        "log_loss_score_train = log_loss(y_train, y_pred_proba_train)\n",
        "\n",
        "# Generate the classification report\n",
        "report_test = classification_report(y_test, y_pred)\n",
        "report_train = classification_report(y_train, y_pred_train)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1score = f1_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "ieUBiw2Xser0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}